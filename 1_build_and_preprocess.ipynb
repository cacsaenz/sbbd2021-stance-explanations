{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and building dataset\n",
    "\n",
    "We perform the described preprocessing, then we build a DataFrame with preprocessed tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuration presets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PREFIX = './datasets'\n",
    "LABEL_MAPPER = {\n",
    "    'antichina': 0,\n",
    "    'antivacina': 1,\n",
    "    'provacina': 2,\n",
    "}\n",
    "\n",
    "LOWERCASE = True\n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_DATASET_SIZE_BY_CLASS = 1000\n",
    "OUTPUT_FILE_NAME = '3000'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "import unidecode\n",
    "import emoji\n",
    "\n",
    "hashtags = [\n",
    "    '#EuVouTomarVacina',\n",
    "    '#VacinaBrasil',\n",
    "    '#VacinaEAmorAoPróximo',\n",
    "    '#VacinaEAmorAoProximo', # without accent\n",
    "    '#VacinaJá',\n",
    "    '#VacinaJa', # without accent\n",
    "    '#VacinaNoBrasil',\n",
    "    '#VacinaParaTodos',\n",
    "    '#VacinasPelaVida',\n",
    "    '#VacinaUrgenteParaTodos',\n",
    "    '#VemVacina',\n",
    "    '#EuNãoVouTomarVacina',\n",
    "    '#EuNaoVouTomarVacina', # withouth accent\n",
    "    '#NãoVouTomarVacina',\n",
    "    '#NaoVouTomarVacina', # without accent\n",
    "    '#VacinaNão',\n",
    "    '#VacinaNao', # without accent\n",
    "    '#VacinaObrigatóriaNão',\n",
    "    '#VacinaObrigatóriaNao', # without accent\n",
    "    '#VacinaObrigatoriaNão', # without accent\n",
    "    '#VacinaObrigatoriaNao', # without accent\n",
    "    '#VachinaNão',\n",
    "    '#VachinaNao', # without accent\n",
    "    '#VachinaNãoPresidente',\n",
    "    '#VachinaNaoPresidente', # without accent\n",
    "    '#VachinaObrigatóriaNão',\n",
    "    '#VachinaObrigatóriaNao', # without accent\n",
    "    '#VachinaObrigatoriaNão', # without accent\n",
    "    '#VachinaObrigatoriaNao', # without accent\n",
    "    '#VacinaChinesaNão',\n",
    "    '#VacinaChinesaNao', # without accent\n",
    "]\n",
    "\n",
    "\n",
    "def removeNonLatinCharacters(tweet):\n",
    "    return \" \".join(\n",
    "        regex.sub(r'[^\\p{Latin}]', u'', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeNumbers(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(r'[0-9]', '', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeEmojis(tweet):\n",
    "    return \" \".join(\n",
    "        emoji.get_emoji_regexp().sub(r'', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeURLs(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(r\"http\\S+|youtu.be\\S+|\\S+.com.br\\S+|bit.ly\\S+|\\S+.com/\\S+|\\S+.co/\\S+|\\S+.org\\S+|\\S+.br/\\S+|\\S+.es/S+\", \"\", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeHashtags(tweet):\n",
    "    for hashtag in hashtags:\n",
    "        # Here, it could be two variations:\n",
    "        # the hashtag symbol (#) + the HT\n",
    "        # the HT only\n",
    "        ht = re.compile(\n",
    "            re.escape(\n",
    "                hashtag\n",
    "            ),\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        tweet = ht.sub('', tweet)\n",
    "        \n",
    "        ht = re.compile(\n",
    "            re.escape(\n",
    "                hashtag[1:]\n",
    "            ),\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        tweet = ht.sub('', tweet)\n",
    "    \n",
    "    # Finally, replace all empty hashtags symbols (#)\n",
    "    return \" \".join(\n",
    "        re.sub(r'# ', '', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeMentions(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(r\"@\\S+\", \"\", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def isValidTweet(tweet):\n",
    "    without_mentions = \" \".join(re.sub(r\"@\", \"\", tweet).split())\n",
    "    if (\n",
    "        len(without_mentions.split()) < 3 or\n",
    "        not without_mentions or\n",
    "        re.search(\"^\\s*$\", without_mentions)\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocessTweet(tweet, antivacina=True):\n",
    "    preprocessed = removeNumbers(\n",
    "        removeEmojis(\n",
    "            removeMentions(\n",
    "                removeURLs(\n",
    "                    removeHashtags(tweet)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    if isValidTweet(preprocessed):\n",
    "        if LOWERCASE:\n",
    "            return preprocessed.lower()\n",
    "        else:\n",
    "            return preprocessed\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is just for testing...\n",
    "antichina = pd.read_csv(f'{PREFIX}/raw_csv/antichina.csv', names=['tweet'])\n",
    "print(antichina['tweet'].values[1435])\n",
    "print(preprocessTweet(antichina['tweet'].values[1435]))\n",
    "print('')\n",
    "\n",
    "antivax = pd.read_csv(f'{PREFIX}/raw_csv/antivacina.csv', names=['tweet'])\n",
    "print(antivax['tweet'].values[37])\n",
    "print(preprocessTweet(antivax['tweet'].values[37]))\n",
    "print('')\n",
    "\n",
    "provax = pd.read_csv(f'{PREFIX}/raw_csv/provacina.csv', names=['tweet'])\n",
    "print(provax['tweet'].values[45])\n",
    "print(preprocessTweet(provax['tweet'].values[45]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset building"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load and build dataset\n",
    "print('Building dataset...\\n')\n",
    "\n",
    "labels = []\n",
    "tweets = []\n",
    "\n",
    "for label in ['antichina', 'antivacina', 'provacina']:\n",
    "    print(f'Preprocessing {label}...')\n",
    "    \n",
    "    group_df = pd.read_csv(f'{PREFIX}/raw_csv/{label}.csv', names=['tweet'])\n",
    "    \n",
    "    for tweet in group_df['tweet'].values:\n",
    "        try:\n",
    "            preprocessed_tweet = preprocessTweet(tweet)\n",
    "        except:\n",
    "            print(f'Failed with {tweet}')\n",
    "            continue\n",
    "\n",
    "        if preprocessed_tweet:\n",
    "            tweets.append(preprocessed_tweet)\n",
    "            labels.append(LABEL_MAPPER[label])\n",
    "    \n",
    "    print('Finished.\\n')\n",
    "    \n",
    "dataset = pd.DataFrame({\n",
    "    'tweet': tweets,\n",
    "    'label': labels,\n",
    "})\n",
    "dataset.to_csv(f'{PREFIX}/complete_dataset.csv', sep=';', columns=['tweet', 'label'], index=False)\n",
    "print(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Complete dataset metrics...')\n",
    "print('# Antichina:', len(dataset[dataset['label'] == 0]))\n",
    "print('# Antivacina:', len(dataset[dataset['label'] == 1]))\n",
    "print('# Provacina:', len(dataset[dataset['label'] == 2]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now build a with the same size of elements of each class\n",
    "sample = pd.concat([\n",
    "    dataset[dataset['label'] == 0].sample(random_state=RANDOM_SEED, n=OUTPUT_DATASET_SIZE_BY_CLASS).reset_index(drop=True),\n",
    "    dataset[dataset['label'] == 1].sample(random_state=RANDOM_SEED, n=OUTPUT_DATASET_SIZE_BY_CLASS).reset_index(drop=True),\n",
    "    dataset[dataset['label'] == 2].sample(random_state=RANDOM_SEED, n=OUTPUT_DATASET_SIZE_BY_CLASS).reset_index(drop=True),\n",
    "]).reset_index(drop=True)\n",
    "sample.to_csv(f'{PREFIX}/{OUTPUT_FILE_NAME}.csv', sep=';', columns=['tweet', 'label'], index=False)\n",
    "sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Sample dataset metrics...')\n",
    "print('# Antichina:', len(sample[sample['label'] == 0]))\n",
    "print('# Antivacina:', len(sample[sample['label'] == 1]))\n",
    "print('# Provacina:', len(sample[sample['label'] == 2]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}